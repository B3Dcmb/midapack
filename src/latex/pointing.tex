
\begin{DoxyItemize}
\item \hyperlink{functionnality}{Functionnality}
\item \hyperlink{data_struct_parall}{Data structure and parallelism}
\item \hyperlink{algorithm}{Communication Algorithm}
\item \hyperlink{example}{Application example} 
\end{DoxyItemize}\section{Functionnality}\label{functionnality}
\hypertarget{functionnality_operations}{}\subsection{Sparse matrix operations}\label{functionnality_operations}
Pointing and Unpointing are usefull operators in the C\-M\-B data analysis. It refers to the applications from time-\/signal-\/domain to sky-\/direction domain. It usually consist in recangular sparse matrices with few non-\/zero values. It is assumed that unpointing matrix has couples time more rows than columns. That means dimension of the time-\/signal domain(tod) is greater than the sky-\/direction domain(map). Pointing matrix is exactly the transposed of the unpointing matrix.

This module contains functionnalities for applying these operations. Which means creating sparse matrices into an efficient sparse matrix format, and also applying matrix vector multiplication or transposed matrix vector multiplication.

The two core functions provided are \-: \begin{DoxyItemize}
\item unpointing product (\hyperlink{mapmat_8c_af757d9249d31d2839b3376ac2e3f5574}{matrix vector multiplication }) \item pointing product (\hyperlink{mapmat_8c_a1a51d7e8153d33045482100bbd07d0a9}{transposed matrix-\/vector multiplication });\end{DoxyItemize}
\hypertarget{functionnality_execution}{}\subsection{Parallel execution}\label{functionnality_execution}
As C\-M\-B data analysis works with large data sets, it require the aibility to execute programs in parallel. Matrices can reach hundred billions of rows, hundred million of columns and be distributed over thousand cores. The aim is to deliver fast and highly scalable operation for sparse rectangular matrices. That's why midapack adopts a customized storage format and several communication schemes for pointing operators.

M\-I\-D\-A\-P\-A\-C\-K parallel execution model is based on distributed memory architecture via the Message Passing Interface(\-M\-P\-I). Most of effort have been done for minmizing communication between processors. Especially we have developed algorithms for collective reduce operations. Moreover, most of the functions can also benefit from a sublevel parallellism using Open\-M\-P. Thus programs build with M\-I\-D\-A\-P\-A\-C\-K can works on big data sets and be runned on big computer in a multi-\/many-\/cores context.

\par
 \section{Data structure and parallelism}\label{data_struct_parall}
\hypertarget{data_struct_parall_input_struct}{}\subsection{Parallel sparse matrix}\label{data_struct_parall_input_struct}
Considering a matrix $A$, parallelism assume $A$ is row-\/distributed over processes. Each processor has into memory m rows of the global matrix. Reciprocally $A^t$ is column-\/distributed, with m columns into memory. That is to say \[ A = \left( \begin{array}{c}A_0 \\A_1\\ \vdots \\ A_{n_{prc}-1} \end{array} \right) \] Reciprocally \[ A^t=(A_0^t, A_1^t, A_2^t, ... A_{n_{prc}-1}^t) \].

As $A$ is a sparse martix, it doesn't store zero values. Furthermore we assume $A$ is exactly nnz nonzerovalues. Then building matrix A only require these non-\/zero values and theirs global columns indices, also called E\-L\-L format. Input data consists in two large tab of size m$\ast$nnz, where rows are concatenated. This input array have to be passed when calling matrix initializtaion function.

To well ballence the load over processes we have to ensure number of rows time number of non-\/zero per row is roughly the same on each processor\hypertarget{data_struct_parall_input_example}{}\subsection{Input data}\label{data_struct_parall_input_example}
The two following examples illustrate the input data needs to build a matrix using \hyperlink{mapmat_8c_aaffd7a76c3cf2834df302d1f844aea3e}{Mat\-Init. } The first one is a sequential, the second consider 2 processors.
\begin{DoxyItemize}
\item sequential case \-: m=8, nnz=2, indices=\mbox{[}0 1 2 4 0 2 0 2 2 3 3 4 1 4 1 3\mbox{]}, values=\mbox{[}1 7 2 8 5 3 5 6 2 9 8 6 1 3 6 4\mbox{]}. \[ A = \left( \begin{array}{ccccc}1&7&0&0&0\\0&0&2&0&8\\5&0&3&0&0\\5&0&2&0&0\\0&0&2&9&0\\0&0&0&8&6\\0&1&0&0&3\\0&6&0&4&0\end{array} \right) \]
\item parallel case over 2 processors \-: input data on processor 0 is m=3, nnz=2, indices=\mbox{[}0 1 2 4 0 2 0 2\mbox{]}, values=\mbox{[}1 7 2 8 5 3 5 6\mbox{]}. Input data on processor 1 is m=4, nnz=2, indices=\mbox{[}2 3 3 4 1 4 1 3\mbox{]}, values=\mbox{[}2 9 8 6 1 3 6 4\mbox{]}. \[ A = \left( \begin{array}{c} A_0 \\ A_1 \end{array} \right) \] \[ A_0 = \left( \begin{array}{ccccc} 1&7&0&0&0\\0&0&2&0&8\\5&0&3&0&0\\5&0&2&0&0\end{array} \right) , A_1 = \left( \begin{array}{ccccc} 0&0&2&9&0\\0&0&0&8&6\\0&1&0&0&3\\0&6&0&4&0\end{array} \right) \] Two remarks about the input data structure (E\-L\-L format) \-:
\item it happens that a row has more or less non-\/zero values that nnz. In this case we can choose the greater nnz and add zero wherever it is necessary with whatever index. For performance we advice to choose an index which has already a value in the row.
\item E\-L\-L format is more general than D\-I\-A format since non-\/zero elements of given row do not need to be ordered. Thus permuting non-\/zero elements of given row in the input data do not change the matrix.
\end{DoxyItemize}\hypertarget{data_struct_parall_intern_struct}{}\subsection{Internal data stucture}\label{data_struct_parall_intern_struct}
The internal structure is more sophisticated than the E\-L\-L format. Especially, to enhance matrix operations performance, a precomputation step reshapes the data structure into several arrays \-: global ordered columns indices, local indices, communication ...

When using Mat\-Init function, precomputation is performed blindly. Nevertheless, for advanced user it is able to initialize a matrix in several steps. This enables to specify differents methods for the precomputations.
\begin{DoxyItemize}
\item set non-\/zero elements indices and values (\hyperlink{mapmat_8c_aaf26c7678367e6757392c03abd22a105}{Mat\-Set\-Indices } \hyperlink{}{Mat\-Set\-Values }),
\item reindex local matrix (\hyperlink{mapmat_8c_ae31f7ccb10cda5c97e49f640feed1ad4}{Mat\-Local\-Shape }),
\item create communication scheme (\hyperlink{}{Mat\-Com\-Shape }).
\end{DoxyItemize}

\par
 \section{Communication Algorithm}\label{algorithm}
Transposed matrix vector multiplication is performed in two steps \-:
\begin{DoxyItemize}
\item Firstly, each processor $ i $ multiply a local vector by a local matrix, $ x_i=A_i^t y_i$.
\item Then processors communicated to update the local result vector, $ x = \sum_{i=0}^{n_{prc}-1} x_i $
\end{DoxyItemize}

The second steps involved to communicate and sum elements of all the each local vectors. When size of the problem or number processors increases, this operation may become a bottleneck. To minimize the computationnal cost of this collective reduce operation, Midapack identifies the minimum parts of elements to communicate between processors. Once it is done, collective communication are executed using one of the custommized algorithms as Ring, Butterfly, Nonblocking, Noempty

The communication algorithm is specified when calling \hyperlink{mapmat_8c_aaffd7a76c3cf2834df302d1f844aea3e}{Mat\-Init } or \hyperlink{}{Mat\-Com\-Shape }. An integer encodes all the communication algorithms (None=0, Ring=1, Butterfly=2, Nonblocking=3 Noempty=4).

\par
 \section{Application example}\label{example}
Here is an application example of a least square problem resolution which is implemented in test\-\_\-pcg\-\_\-mapmat.\-c . Considering a problem formulated as $ A^t A x = A^t b $, Solution x can be compute iteratively using conjutgate gradient. Instead computing and storing the whole $A^tA$ matrix, we apply succesively pointing, $A^t$, and unpointing products, $A$, at each iterate.

Classic gradient conjugate algorithm has been slightly modified. As we explain $A^t$ and $A$ are applied succesively. Furtherwise dotproduct operations in the overlapped domain have been moved in the distributed domain. Espacially we use relation \-: $< A^t y, x > = < y , Ax > $.

Algorithm needs into memory 6 vectors \-:
\begin{DoxyItemize}
\item 3 in ovelapped domain(x, gradient, direction),
\item 3 in distributed domain.
\end{DoxyItemize}

Complexity, counting operations at each iterate, is detailled as follow \-:
\begin{DoxyItemize}
\item 4 produits scalaires dans le domaine temporelle (communication = single M\-P\-I\-\_\-\-Allreduce),
\item 3 axpy dans le domaine de la carte (no communication),
\item 3 multiplication par $A$ (no communication),
\item 1 multiplication par $A^t$ (communication = M\-I\-D\-A\-P\-A\-C\-K Communication scheme).
\end{DoxyItemize}


\begin{DoxyCode}
    Mat A;
    \textcolor{keywordtype}{double} *x, *g, *d;
    \textcolor{keywordtype}{double} *Ax\_b, *Ag, *Ad; 
   
    MatCreate(&A, m, nnz, MPI\_COMM\_WORLD);      \textcolor{comment}{//allocate matrix tabs}
    MatSetIndices(&A, m*nnz, 0, nnz, indices);  \textcolor{comment}{//copy indices into matrix
       structure}
    MatSetValues(&A, m*nnz, 0, nnz, values);    \textcolor{comment}{//copy values into matrix
       structure}
   
    MatLocalShape(&A, SORT\_FLAG, OMP\_FLAG);     \textcolor{comment}{//reindex data structure}
   
    MatComShape(&A, COM\_SCHEME\_FLAG);           \textcolor{comment}{//build communication pattern }
   
    \textcolor{comment}{//conjugate gradient initialization}
    \textcolor{comment}{//allocate vectors (overlapped domain) }
    g  = (\textcolor{keywordtype}{double} *) malloc(A.lcount*\textcolor{keyword}{sizeof}(\textcolor{keywordtype}{double}));      \textcolor{comment}{//g (gradient)}
    d  = (\textcolor{keywordtype}{double} *) malloc(A.lcount*\textcolor{keyword}{sizeof}(\textcolor{keywordtype}{double}));      \textcolor{comment}{//d (direction)  }
    \textcolor{comment}{//allocate vector (distributed domain) }
    Ax\_b = (\textcolor{keywordtype}{double} *) malloc(m*\textcolor{keyword}{sizeof}(\textcolor{keywordtype}{double}));           \textcolor{comment}{//Ax\_b = Ax-b}
    Ad = (\textcolor{keywordtype}{double} *) malloc(m*\textcolor{keyword}{sizeof}(\textcolor{keywordtype}{double}));             \textcolor{comment}{//Ad = A d}
    Ag = (\textcolor{keywordtype}{double} *) malloc(m*\textcolor{keyword}{sizeof}(\textcolor{keywordtype}{double}));             \textcolor{comment}{//Ag = A g}
   
    MatVecProd(&A, x, Ax\_b, 0);           \textcolor{comment}{//Ax\_b = Ax-b}
    \textcolor{keywordflow}{for}(i=0; i<m; i++)                    \textcolor{comment}{// }
      Ax\_b[i] = Ax\_b[i]-b[i];             \textcolor{comment}{//}
   
    TrMatVecProd(&A, Ax\_b, d, 0);         \textcolor{comment}{//Ad = A d =  A A^t(Ax-b)}
    MatVecProd(&A, d, Ad, 0);             \textcolor{comment}{//}
   
    resnew=0.0;                           \textcolor{comment}{//initial residu, resnew =
       ||A^t(Ax-b)|| = <Ax\_b, Ad>}
    localreduce=0.0;                      \textcolor{comment}{//}
    \textcolor{keywordflow}{for}(i=0; i<m; i++)                    \textcolor{comment}{//         }
      localreduce+=Ax\_b[i]*Ad[i];         \textcolor{comment}{//}
    MPI\_Allreduce(&localreduce, &resnew, 1, MPI\_DOUBLE, MPI\_SUM, MPI\_COMM\_WORLD
      );
   
    \textcolor{comment}{//conjugate gradient iterate}
    \textcolor{keywordflow}{for}(k=0; k<KMAX ; k++)\{               \textcolor{comment}{//begin loop}
   
      alpha=0.0;                          \textcolor{comment}{//alpha = <Ad, Ax\_b>}
      localreduce=0.0;                    \textcolor{comment}{//}
      \textcolor{keywordflow}{for}(i=0; i<m; i++)                  \textcolor{comment}{//         }
        localreduce+=Ad[i]*Ax\_b[i];       \textcolor{comment}{//}
      MPI\_Allreduce(&localreduce, &alpha, 1, MPI\_DOUBLE, MPI\_SUM, 
      MPI\_COMM\_WORLD);
   
      gamma=0.0;                          \textcolor{comment}{//gamma = <Ad, Ad>}
      localreduce=0.0;                    \textcolor{comment}{//}
      \textcolor{keywordflow}{for}(i=0; i<m; i++)                  \textcolor{comment}{//         }
        localreduce+=Ad[i]*Ad[i];         \textcolor{comment}{//}
      MPI\_Allreduce(&localreduce, &gamma, 1, MPI\_DOUBLE, MPI\_SUM, 
      MPI\_COMM\_WORLD);
   
      \textcolor{keywordflow}{for}(j=0; j<A.lcount; j++)           \textcolor{comment}{// x = x + (alpha/gamma) d }
        x[j] = x[j] - (alpha/gamma)* d[j];\textcolor{comment}{//}
   
      MatVecProd(&A, x, Ax\_b, 0);         \textcolor{comment}{//Ax\_b = Ax-b}
      \textcolor{keywordflow}{for}(i=0; i<m; i++)                  \textcolor{comment}{// }
        Ax\_b[i] = Ax\_b[i]-b[i];           \textcolor{comment}{//}
   
      TrMatVecProd(&A, Ax\_b, g, 0);       \textcolor{comment}{//g  = A^t(Ax-b)}
      MatVecProd(&A, g, Ag, 0);           \textcolor{comment}{//Ag = AA^t(Ax-b)}
   
      resold=resnew;                      \textcolor{comment}{//residu = ||g|| = <Ax-b, Ag>}
      resnew=0.0;                         \textcolor{comment}{//}
      localreduce=0.0;                    \textcolor{comment}{//}
      \textcolor{keywordflow}{for}(i=0; i<m; i++)                  \textcolor{comment}{//         }
        localreduce+=Ax\_b[i]*Ag[i];       \textcolor{comment}{//}
      MPI\_Allreduce(&localreduce, &resnew, 1, MPI\_DOUBLE, MPI\_SUM, 
      MPI\_COMM\_WORLD);
   
      beta=0.0;                           \textcolor{comment}{//beta = <Ag, Ad>}
      localreduce=0.0;                    \textcolor{comment}{//}
      \textcolor{keywordflow}{for}(i=0; i<m; i++)                  \textcolor{comment}{//         }
        localreduce+=Ag[i]*Ad[i];         \textcolor{comment}{//}
      MPI\_Allreduce(&localreduce, &beta, 1, MPI\_DOUBLE, MPI\_SUM, MPI\_COMM\_WORLD
      );
   
      \textcolor{keywordflow}{if}(resnew<tol)                       \textcolor{comment}{//convergence test}
         \textcolor{keywordflow}{break};
       
      \textcolor{keywordflow}{for}(j=0; j<A.lcount; j++)           \textcolor{comment}{//d = -g + (beta/gamma) d}
        d[j]= -g[j] + (beta/gamma)*d[j];  \textcolor{comment}{// }
   
      MatVecProd(&A, d, Ad, 0);           \textcolor{comment}{//Ad = A d}
    \}
\end{DoxyCode}


More information about pointing operator are detailled, in the \hyperlink{mapmat_8c}{pointing function synposis }

\par
 