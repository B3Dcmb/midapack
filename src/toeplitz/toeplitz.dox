/*
** File: toeplitz.dox, version 1.1b, July 2012  
** This file is the main part of the Toeplitz algebra module 
**
** Project:  Midapack library, ANR MIDAS'09 - Toeplitz Algebra module
** Purpose:  Provide Toeplitz algebra tools suitable for Cosmic Microwave Background (CMB)
**           data analysis.
**
** Authors:  Frederic Dauvergne, Pierre Cargemel, Maude Le Jeune, Antoine Rogier, 
**           Radek Stompor (APC, Paris)
**
** 
** 
** Copyright (c) 2010-2012 APC CNRS Universit√© Paris Diderot
** 
** This program is free software; you can redistribute it and/or modify
** it under the terms of the GNU Lesser General Public License as published by
** the Free Software Foundation; either version 3 of the License, or
** (at your option) any later version.
** This program is distributed in the hope that it will be useful,
** but WITHOUT ANY WARRANTY; without even the implied warranty of
** MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
** GNU Lesser General Public License for more details.
** 
** You should have received a copy of the GNU General Public License
** along with this program; if not, see http://www.gnu.org/licenses/lgpl.html
**
** For more information about ANR MIDAS'09 project see
** http://www.apc.univ-paris7.fr/APC_CS/Recherche/Adamis/MIDAS09/index.html
*/
///
/// \page toeplitz Toeplitz algebra documentation
///
/// - \subpage toeplitz_intro
/// - \subpage toeplitz_functionality
/// - \subpage toeplitz_algo
/// - \subpage toeplitz_progmodels
/// - \subpage toeplitz_datadistr
/// - \subpage toeplitz_loadbalancing
/// - \subpage toeplitz_avail
/// - \subpage toeplitz_install
/// - \subpage toeplitz_example
///
/// \page toeplitz_intro Introduction
/// The purpose of the Toeplitz algebra package of the MIDAPACK libary is to provide efficient
/// massively, parallel routines, performing products of some special structured matrices 
/// and an arbitrary matrix. The special matrices considered here are related to \b Toeplitz
/// matrices.
/// 
/// Toeplitz matrices are ubiquitous in the CMB data analysis as they describe
/// correlation properties of stationary time-domain processes (usually instrumental noise).
/// The matrices relevant are therefore \b symmetric and \b non-negative definite. They are also
/// \b band-diagonal as the noise correlation length, i.e., the band-width in the parlance of
/// Toeplitz algebra, is typically much shorter than length of the data. 
/// A useful and important generalization of those include : 
/// - \b symmetric, \b Toeplitz \b block-diagonal matrices -
/// describing piece-wise stationary processes, each of the blocks is in turn a symmetric, band-diagonal
/// Toeplitz matrix, which can be different for the different blocks. The performance of the routines 
/// included in the package is expected to be the best, whenever the bandwidth of each block is much smaller
/// than its size.
///
/// - \b symmetric, \b Toeplitz \b block-diagonal, \b gappy matrices - which are just  symmetric, Toeplitz
/// block-diagonal matrices but with some of their rows (and corresponding columns) removed. Such matrices describe
/// piece-wise stationary processes, which have some short sequences of samples, referred hereafter as \b gaps,
/// removed. The gaps are common in the analysis of any real CMB data sets and can arise due to cosmic rays hits
/// or some short-term instrumental transients. If a gap happens to be longer than the matrix correlation length
/// accounting on it, i.e., removing relevant rows and columns of the initial matrix, will result in a new
/// block of the matrix, which remains symmetric, block-diagonal.
///
/// The library provides distributed (MPI) and sequential/multithreaded (openMP) routines, which are based on common
/// low level sequential/openMP functions. The Fourier Transforms are performed with help of the FFTW library: <http://www.fftw.org/> .
///
/// The overall structure of the library is as follows:
///
/// \image html "diag_toeplitz_structure.png" "Overall structure of the Toeplitz package of the MIDAPACK library."
///
///
///
/// \page toeplitz_functionality Functionality and definitions
/// The Toeplitz algebra package described here provides functionality for calculating \b products of
/// a \b generalized \b Toeplitz \b matrix (understood as one of those defined \ref toeplitz_intro "earlier") and a \b general \b matrix.
/// The latter is referred to hereafter typically as a \b data \b matrix. This is the latter matrix, which defines the reference
/// for the global indexing scheme adopted throughout the package, which is based on its global row number. The data matrices
/// are always stored as vectors in the \b column-wise order.
/// 
/// In MPI-cases the data matrices are assumed to be
/// distributed over the processes as defined in \ref toeplitz_datadistr "Section on data distribution".
///
/// \b Toeplitz \b matrices are defined by a first row of the Toeplitz matrix trimmed to its half-bandwidth \f$+1\f$ denoted hereafter as
/// \f$\lambda\f$, which one therefore includes half-bandwidth and the diagonal element), and three
/// integer numbers giving \f$\lambda\f$, indices of the first and last row to which the matrix should be applied. The last 
/// two numbers are global row numbers of the data matrix. The size of
/// the Toeplitz matrix is then implictly given as, \c last_row_index - \c first_row_index + \c 1. We will refer to an interval
/// defined by these two indices as \b Toeplitz \b matrix \b range.
///
/// The list of specific functions provided is as follows: 
/// \arg symmetric Toeplitz matrix-matrix product  (\subpage toeplitz_funct_stmm);
/// \arg symmetric Toeplitz  block-diagonal matrix-matrix product (\subpage toeplitz_funct_stbmm);
/// \arg symmetric, gappy, Toeplitz block-diagonal matrix-matrix product (\subpage toeplitz_funct_gstbmm).
///
///
/// \page toeplitz_funct_stmm stmm routines
///
/// \c stmm routines multiply a symmetric banded Toeplitz matrix by the data matrix. 
/// The Toeplitz matrix size is assumed to be equal to that of the data matrix. If this is not the case use \c stbmm routines
/// instead. If the data matrix is distributed over multiple processes
/// the Toeplitz matrix has to be defined on each of them and has to be the same on each of them, including the first and the 
/// last row indices used to defined its region.
///
/// \page toeplitz_funct_stbmm stbmm routines
///
/// \c stbmm routines multiply a symmetric, block-diagonal matrix, with Toeplitz banded blocks by a data matrix. This operation
/// is in fact construed as a product of a series of Toeplitz blocks by the data matrix. The Toeplitz blocks are each defined
/// as any Toeplitz matrix (see, e.g., \ref toeplitz_functionality above) and have in particular defined a range - an interval of the data matrix
/// rows to which it should be applied. \b The \b ranges \b of \b any \b two \b Toeplitz \b matrices \b must \b not \b overlap. 
/// The limits of the interval can be looked at as defining the position of each Toeplitz block within
/// a single Toeplitz-block diagonal matrix of the size equal that of a number of rows of the data matrix. What the routine then
/// does it is to multiply each block by a respective subblock of the data matrix. The rows of the data matrix, which do not 
/// belong to the interval of any block are copied unchanged, meaning that the corresponding blocks of the Toeplitz matrix are
/// implicitly assumed to be equal to \f$ 1 \f$.
///
/// In the MPI implementation each processor needs only those of all the Toeplitz blocks which have ranges 
/// overlapping with part of the data matrix assigned to it. If more are defined on the input they are ignored.
/// Note that this is the user's responsibility to ensure that the Toeplitz matrices are assigned to different processes
/// in a consistent way meaning they represent a product of a series of diagonal Toeplitz blocks. It is important to observe
/// that the block distribution will in general depend on the assumed distribution pattern for the data matrix. This is discussed
/// in the \subpage toeplitz_funct_stbmm_examples "examples" below.
///
/// If a Toeplitz block corresponds to the data matrix parts assigned to two (or more) processes, the Toeplitz block parameters 
/// have to be the same on all implicated processes,
/// as each process will use them to define the amount of data which needs communicate to its neighbors. In general, each process
/// calculates products of the Toeplitz blocks by the corresponding part of the data matrix.
/// If a Toeplitz block corresponds to data of more than one process then the communication is used to copy necessary data to enable such a multiplication locally. 
///
/// \ref toeplitz_funct_stbmm_examples "Examples."
/// 
/// \page toeplitz_funct_stbmm_examples Examples
///
/// \b Example \b #1: Column-wise data distribution.
///
/// \image html fig_stbmm_columnwise.png 
///
/// This figure illustrates the operations performed by the \c mpi_stbmm routine. On the input the routine requires a definition of the Toeplitz-blocks and the data matrix. The latter
/// is assumed to be distributed in the column-wise order - here between 3 MPI-processes, as marked by three different colors. The routine will multiply each Toeplitz-block by the corresponding set of rows of
/// each column of the data matrix. The rows of the data matrix which do not correspond to any of the blocks will be copied without change to the resulting matrix.
/// The dashed lines mark the data divisions between the processes which will require some MPI communication to be performed. The communication will affect only the data of a single column
/// which happens to be shared between two processes. We note that for the result to be correct
/// each of the three processes considered here has to have both Toeplitz blocks defined in the same way on each of them.
///
/// \b Example \b #2: Row-wise data distribution.
///
/// \image html fig_stbmm_rowwise.png 
///
/// This figure corresponds to the analogous operation as the one in Example #1 just assuming that the data matrix is distributed between 3 processes in the row-wise order.
/// The dashed lines indicate communication instances between the processes. Note that unlike in the previous case this time the data for \a all columns have to be exchanged.
/// In this case for the operation to be consistent process #0 needs only the first Toeplitz block in its memory, process #2 - only the second, and process #1 - both.
///
/// \page toeplitz_funct_gstbmm gstbmm routines
///
/// \c gstbmm routines multiply a symmetric Toeplitz block diagonal matrix, i.e., as used by \c stbmm routines, but with some sets of rows and corresponding columns removed, by an arbitrary data matrix.
/// Such an \a effective matrix, referred hereafter as a \c gstb (a \a gappy, \a symmetric, \a Toeplitz block matrix), in general will not be made of only Toeplitz blocks anymore. (See the \subpage toeplitz_funct_gstbmm_example "example" below). On the input this matrix
/// is however still represented as a corresponding \c stb matrix complemented by a list of column (or row)
/// which are to be neglected (or effectively excised). The operation is then construed also as a \c stbmm operation, which the \c sbt matrix representating the \c gstb and, and the data matrix with the 
/// corresponding rows set to zero. Note that the data matrix is assumed to have all the rows on the input including the ones to be neglected. 
///
/// On the output the routine will produce a matrix of the same type as the input one with the values in the rows to be neglected set to zero.
///
/// \ref toeplitz_funct_gstbmm_example "Example."
/// 
/// \page toeplitz_funct_gstbmm_example Example
///
/// \image html fig_gstbmm.png
///
/// The figure shows a product of a Toeplitz block matrix with a number of columns and rows to be excised by a data matrix. The excised columns (and rows) are marked by the dashed grids and correspond to the grayish
/// areas of the input and output data matrices. These grayish sets of rows will be set to zero in the final result. We note that two of the three excised intervals, hereafter called \a gaps, of columns are broad enough that
/// they effectively lead only to change of the size of the Toeplitz blocks including splitting one of them into two smaller ones. The third (rightmost) gap however destroys the Toeplitz structure of the second block. Indeed 
/// the \a effective matrix, by which white shaded part of the data matrix are multiplied by, corresponds to only dark blue areas, and does not have a Toeplitz block structure.
///
/// \page toeplitz_algo Numerical algorithms
///
/// The package implements two algorithms for performing the operations. 
///
/// The \b first algorithm is based on a \a shift-and-overlap approach, where
/// a product of a single band-diagonal Toeplitz matrix by an arbitrary matrix is done as a sequence of products of a submatrix of
/// the initial Toeplitz matrix by overlapping blocks of the arbitrary matrix. Each of the latter products is performed in turn by
/// embedding the Toeplitz subblock in a minimal circulant matrix and performing the multiplication via Fast Fourier transforms.
/// The size of the subblock can be set appropriately to optimize the calculation and typically is a function of the bandwith. Denoting by \f$ \lambda\f$
/// a half bandwith, i.e., the full bandwidth is \f$ 2 \lambda + 1 \f$, the overall complexity of the operation is \f${\cal O}( n \ln \lambda)\f$, where \f$ n \f$ is the 
/// size of the initial Toeplitz matrix. 
///
/// Check \subpage toeplitz_algo_shiftoverlap for more details.
///
/// The \b second algorithm is just a \a direct \a real \a space multiplication of a Toeplitz matrix by an arbitrary one. This approach has complexity
/// \f$ {\cal O}( n \lambda)\f$ but much better prefactors and therefore can have superior performance over the previous one for very narrow bands.
///
/// All the other operations implemented in the package are then expressed as the product of a Toeplitz matrix times a general matrix. This may
/// in turn require on occasions some data objects reformatting (or as called hereafter - reshaping) operations.
///
/// The inter-process communication is required whenever the same Toeplitz matrix (or a Toeplitz block for Toeplitz block-diagonal cases) is to be applied to
/// a segment of the data distributed over more than one process. These are implemented using MPI calls.
///
/// More details can be found here \subpage toeplitz_algo_communication 
///
/// \page toeplitz_algo_communication MPI communication patterns
/// The inter-process communication is needed in the MPI routines of the package whenever boundaries of the distributed data matrix
/// do not coincide with those of the Toeplitz block. 
/// The communication pattern is \b local in a sense that it involves only neighboring processes and is therefore 
/// expected to scale well with a number of MPI processes (and it indeed does in the regime in which the tests have been done.)
/// It involves each process sending to and receiving from a neighboring process a vector of data of the length defined by the 
/// half-bandwidth of the Toeplitz block, \f$ \lambda\f$, shared between them. This provides sufficient information to enable
/// each process to compute a part of the Toeplitz-vector product corresponding to its input data on its own without any need for further data exchanges. In particular we note that all the FFT calls used by the package are either sequential or threaded.
///
/// The communication pattern as implemented is either \a non-blocking and then instituted with help of \c MPI_Isend and \c MPI_Irecv calls used twice to send to and receive from left and right, i.e.,
///
/// \snippet toeplitz_block.c communication non-blocking example
///
/// what is followed by a series of respective \c MPI_Wait calls, i.e.,
///
/// \snippet toeplitz_block.c communication Wait example
///
/// or \a blocking implemented with help \c MPI_Sendrecv calls, i.e.,
///
/// \snippet toeplitz_block.c communication blocking example
///
/// The choice between the two is made with help of the global flag \c FLAG_BLOCKINGCOMM, which by default is set to 0 (non-blocking communication). 
///
/// \page toeplitz_algo_shiftoverlap Shift and overlap algorithm
///
/// This algorithm exploits explicitly the fact that considered Toeplitz matrices are band-diagonal with a narrow band, i.e., \f$\lambda \ll n\f$ and 
/// cuts the complexity of the operation down to \f$ {\cal O|}(n \ln \lambda)\f$ from \f${\cal O}(2 n \ln 2 n)\f$, where the latter is obtained assuming
/// embedding of the full Toeplitz matrix of a rank \f$ n\f$ into a circulant matrix of a twice larger rank and performing the product via Fast Fourier
/// transforms.
/// 
/// The shift and overlap algorithm performs the same task as a series of products of a smaller circulant matrix with a rank \f$ b\f$, where \f$ b > 2\lambda\f$,
/// by a corresponding, overlapping segments of the arbitrary matrix. The circulant matrix embeds a Toeplitz matrix, which is just the inital matrix trimmed to the size \f$ b\f$. 
/// The schematic of the algorithm is shown in the figure below.
///
/// \image html Fig_shiftOverlap.png "Pictorial representation of the shift and overlap algorithm."
///
/// Here a product of a Toeplitz matrix marked in black by a vector is split into three products of a circulant matrix
/// of a rank \f$ b\f$ by three overlapping segments of the input vector. Each product is marked by a different color, however
/// the circulanr matrix by which the vector segments are multiplied is always the same. The overlaps are clearly necessary to
/// avoid contributions from the circulant corners of the matrix. At the end the entries of the final vector which are biased
/// by the corner contributions are removed
/// from the result and the remainders combined together. Note that the edge segments need to be padded by zeros. The padding
/// is done in the way that the circulant block size used is always the same. This helps to save the time needed for
/// FFT related precomputation (FFT plans etc) and optimize a number of required FFTs. 
///
/// The generalization of the algorithm for the case of a general matrix instead of a vector, as shown in the figure,
/// is straightforward. We note that
/// each of the elemental products of the circulant matrix times a general matrix subblock could in principle be performed 
/// in a single step using an FFT, which permits a computation of many identical FFTs simultanenously rather than it being
/// implemented as a series of the products of the circulant matrix by subblock columns. Given that the gain in using 
/// multi-vector is not clear in current implementations of the FFTs we looked at and, if present, it is probably at the 
/// best limited to a relatively small number of the vectors, the adopted solution in the package represents the product 
/// of the circulant block by a general matrix subblocks as series of products each involving the circulant matrix by a 
/// subset of all columns of the general matrix. The number of the columns is set by the \c toeplitz_init routine.
///
/// In general given the size of the input problem \f$n\f$ the cost of the computation is:
/// \f[
///      n/(n-2\lambda)\times b \ln b \sim n \ln b \sim n \ln \lambda
/// \f]
/// where the first factor of the leftmost term gives a number of products to be performed and the latter the cost of each 
/// of them. Here we did not account on any gains from a multi-vector FFT, e.g., we have assumed that a simulatenous FFT 
/// of \f$k\f$-vectors is as costly as \f$k\f$ FFTs of a single vector.
///
/// \page toeplitz_progmodels Programming models
/// The Toeplitz algebra library routines allow the user to take advantage of both
/// \b multithreaded and \b memory-distributed programming paradigms and are therefore
/// adapted to run efficiently on heteregeneous computer architectures. The multithreading
/// is implemented using \b openMP directives, while distributed programming uses \b MPI. Both shared
/// and/or distributed parallelism can be switched of, at the compilation time, if
/// so desired. Moreover, the user has always access to two versions of each of the routines:
/// openMP/MPI and openMP-only.
///
/// \page toeplitz_datadistr Data distribution
/// In the memory-distributed (MPI) running modes, the data input matrix is assumed to be distributed in between
/// the MPI processes (nodes, processors, etc). 
/// The library routines allow essentially for two different data distributions as well as one inermediate option.
///
/// \image html Fig_ToeplitzDataDistribution.png "Illustrations of three different data layouts accepted by the MPI routines. The data matrix shown here is divided between 4 MPI-processes using, from \a left to \a right the \c column-wise, \c row-wise, and \c hybrid (with \a k = 2) distribution."
/// 
/// The first distribution is called hereafter a \b column-wise distribution. In this case the data matrix is treated
/// as a vector made of columns of the data matrix concatenated together. A valid data distribution can be then 
/// nearly any partition of the vector into consecutive segments, which are then assigned one-by-one to the processes.
/// It is then assumed that the neighboring processes receive consecutive segments. Moreover, each process has to
/// have at least as many data points as a half-bandwith of a Toeplitz block corresponding to them, if it has only
/// one Toeplitz block assigned, which does not start or end within the data ranges.
///
/// The second distribution is called hereafter a \b row-wise distribution and it corresponds to dividing the data matrix
/// into subblocks with a number of columns as in the full matrix. This time neighboring processess have to have blocks 
/// corresponding to the consecutive rows of the data matrix and each process has to have at least as many rows as
/// the band-width of the corresponding Toeplitz blocks, unless one of the Toeplitz blocks assigned to that set of rows
/// starts or end within the rows interval.
///
/// The \b hybrid data distribution first represents the data matrix as a matrix of \f$ k \f$ columns, where \f$ 1 \le k \le \f$ \a # \a of \a columns \a of \a the \a data \a matrix. This is obtained by concatenating first \f$ k \f$ columns of the data matrix, by following \f$ k\f$ etc ... - note that the total number of columns of the data matrix has to divide by \f$ k\f$ - and then 
/// chopping such a matrix into segments assigned to different MPI process. The requirements as above also apply.
///
/// What data layout is used is defined by the input parameters of the MPI routines.
///
/// For all the routines of the package, \b the \b layout \b of \b the \b output \b coincides \b with \b that \b of \b the \b input.
///
/// \page toeplitz_loadbalancing  Load balancing
///
/// In the case of the MPI routines the load balancing will be generally dependent on the adopted data distribution and
/// therefore it is left to the user to select the latter to ensure the former. On the other hand, the library is designed
/// to work, albeit potentially not very efficiently, whether such a goal is achieved or not and therefore also in circumstances far from the load balancing.
///
/// \page toeplitz_avail Availability and bug tracking
/// You can download the last release from the official website of the ANR-MIDAS'09 project at
/// http://www.apc.univ-paris7.fr/APC_CS/Recherche/Adamis/MIDAS09/software/midapack/
///
/// Please report any bugs via bug tracker at:
/// http://code.google.com/p/cmb-da-library/
/// 
/// \page toeplitz_install Installation
///
///This software is reported to work on several Linux distributions and should work on any
///modern Unix-like system after minimal porting efforts.
///
///The source code is delivered in a set of directories :
///
/// - The /src directory contains the sources files for the core library. It's composed by the
///differents modules of the MIDAS CMB DA library (please refer to the website for more details).
///You can directly compile theses files and link the generated binaries with your own program.
///
/// - The /test directory contains some Utility/demonstration programs to show some examples of
///how to use the library fonctionnalities.
///
/// \page toeplitz_example User example
///
/// Here is a short example showing how to use it: 
/// \code
/// // sequential use
/// fftw_complex *V_fft, *T_fft;
/// double *V_rfft;
/// fftw_plan plan_f, plan_b;
/// tpltz_init(v1_size, lambda , &nfft, &blocksize, &T_fft, T, &V_fft, &V_rfft, &plan_f, &plan_b);
/// stmm(V, n, m, id0, local_V_size, T_fft, lambda, V_fft, V_rfft, plan_f, plan_b, blocksize, nfft);
/// tpltz_cleanup(&T_fft, &V_fft, &V_rfft, &plan_f, &plan_b);
/// \endcode
/// \code
/// // MPI use
/// MPI_Scatterv(V, nranks, displs, MPI_DOUBLE, Vrank, maxsize, MPI_DOUBLE, 0, MPI_COMM_WORLD); 
/// flag_stgy_init_auto(&flag_stgy);
/// mpi_stbmm(&Vrank, nrow, m, m_rowwise, tpltzblocks, nb_blocks, nb_blocks, id0, local_V_size, flag_stgy, MPI_COMM_WORLD);
/// MPI_Gatherv(Vrank, nranks[rank], MPI_DOUBLE, TV, nranks, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD);
/// \endcode
///

