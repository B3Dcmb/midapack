/// \page pointing Pointing operations documentation
///
/// - \subpage functionnality
/// - \subpage data_struct_parall
/// - \subpage algorithm
/// - \subpage example
///
/// \page functionnality Functionnality
///
/// \section operations Sparse matrix operations
/// Pointing and Unpointing are usefull operators in the CMB data analysis.
/// It refers to the applications from time-signal-domain to sky-direction domain.
/// It usually consist in recangular sparse matrices with few non-zero values. 
/// It is assumed that unpointing matrix has couples time more rows than columns.
/// That means dimension of the time-signal domain(tod) is greater than the sky-direction domain(map).
/// Pointing matrix is exactly the transposed of the unpointing matrix.
///
/// This module contains functionnalities for applying these operations.
/// Which means creating sparse matrices into an efficient sparse matrix format,
/// and also applying matrix vector multiplication or transposed matrix vector multiplication.
///
/// The two core functions provided are :
/// \arg unpointing product (@link MatVecProd matrix vector multiplication @endlink)
/// \arg pointing product (@link TrMatVecProd transposed matrix-vector multiplication @endlink);
///
/// \section execution Parallel execution
/// As CMB data analysis works with large data sets, it require the aibility to execute programs in parallel.
/// Matrices can reach hundred billions of rows, hundred million of columns and be distributed over thousand cores. 
/// The aim is to deliver fast and highly scalable operation for sparse rectangular matrices.
/// That's why midapack adopts a customized storage format and several communication schemes for pointing operators.
///
/// MIDAPACK parallel execution model is based on distributed memory architecture via the Message Passing Interface(MPI).
/// Most of effort have been done for minmizing communication between processors.
/// Especially we have developed algorithms for collective reduce operations. 
/// Moreover, most of the functions can also benefit from a sublevel parallellism using OpenMP.
/// Thus programs build with MIDAPACK can works on big data sets and be runned on big computer in a multi-many-cores context.
///
/// @n
/// \page data_struct_parall Data structure and parallelism
/// 
/// \section input_struct Parallel sparse matrix
/// Considering a matrix \f$A\f$, parallelism assume \f$A\f$ is row-distributed over processes. 
/// Each processor has into memory m rows of the global matrix.
/// Reciprocally \f$A^t\f$ is column-distributed, with m columns into memory.
/// That is to say \f[ A = \left( \begin{array}{c}A_0 \\A_1\\ \vdots \\ A_{n_{prc}-1} \end{array} \right) \f] 
/// Reciprocally  \f[ A^t=(A_0^t, A_1^t, A_2^t, ... A_{n_{prc}-1}^t) \f]
///
/// As \f$A\f$ is a sparse martix, it doesn't store zero values. 
/// Furthermore we assume \f$A\f$ is exactly nnz nonzerovalues. 
/// Then building matrix A only require these non-zero values and theirs global columns indices, also called ELL format.
/// Input data consists in two large tab of size m*nnz, where rows are concatenated.
/// This input array have to be passed when calling matrix initializtaion function. 
///
/// To well balance the load over processes we have to ensure number of rows time number of non-zero per row is roughly the same on each processor.		
///
/// \section input_example Input data
///
/// The two following examples illustrate the input data needs to build a matrix using @link MatInit MatInit. @endlink
/// The first one is a sequential, the second consider 2 processors.
/// - sequential case : m=8, nnz=2, indices=[0 1 2 4 0 2 0 2 2 3 3 4 1 4 1 3], values=[1 7 2 8 5 3 5 6 2 9 8 6 1 3 6 4].
/// \f[ A = \left( \begin{array}{ccccc}1&7&0&0&0\\0&0&2&0&8\\5&0&3&0&0\\5&0&2&0&0\\0&0&2&9&0\\0&0&0&8&6\\0&1&0&0&3\\0&6&0&4&0\end{array} \right) \f] 
/// - parallel case over 2 processors : input data on processor 0 is m=3, nnz=2, indices=[0 1 2 4 0 2 0 2], values=[1 7 2 8 5 3 5 6]. 
/// Input data on processor 1 is m=4, nnz=2, indices=[2 3 3 4 1 4 1 3], values=[2 9 8 6 1 3 6 4].
/// \f[ A = \left( \begin{array}{c} A_0 \\ A_1 \end{array} \right) \f] 
/// \f[ A_0 = \left( \begin{array}{ccccc} 1&7&0&0&0\\0&0&2&0&8\\5&0&3&0&0\\5&0&2&0&0\end{array} \right) , 
/// A_1 = \left( \begin{array}{ccccc} 0&0&2&9&0\\0&0&0&8&6\\0&1&0&0&3\\0&6&0&4&0\end{array} \right)   \f] 
/// Two remarks about the input data structure (ELL format) :
///     - It happens that a row has more or less non-zero values than nnz.
///     In this case we can choose the greater nnz and add zero wherever it is necessary with whatever index.
///     For performance we advise to choose an index which has already a value in the row.
///     - ELL format is more general than DIA format since non-zero elements of given row do not need to be ordered.
///     Thus permuting non-zero elements of given row in the input data do not change the matrix.
///
/// \section intern_struct Internal data stucture
///
/// The internal structure is more sophisticated than the ELL format. 
/// Especially, to enhance matrix operations performance, a precomputation step reshapes the data structure into several arrays : 
/// global ordered columns indices,  local indices, communication ...
///
///  
/// 
/// When using MatInit function, precomputation is performed blindly.
/// Nevertheless, for advanced users it is possible to initialize a matrix in several steps.
/// This enables to specify differents methods for the precomputations.
/// - set non-zero elements indices and values (@link MatSetIndices MatSetIndices @endlink @link MatSetValues MatSetValues @endlink),
/// - reindex local matrix (@link MatLocalShape MatLocalShape @endlink),
/// - create communication scheme (@link MatComShape MatComShape @endlink).
/// 
/// @n 
/// \page algorithm Communication Algorithm
///
/// Transposed matrix vector multiplication is performed in two steps :	
///  - Firstly, each processor \f$ i \f$ multiply a local vector by a local matrix, \f$ x_i=A_i^t y_i\f$.
///  - Then processors communicated to update the local result vector, \f$ x = \sum_{i=0}^{n_{prc}-1} x_i \f$
///
/// The second steps involved to communicate and sum elements of all the each local vectors.
/// When size of the problem or number processors increases, this operation may become a bottleneck.
/// To minimize the computationnal cost of this collective reduce operation, 
/// Midapack identifies the minimum parts of elements to communicate between processors.
/// Once it is done, collective communication are executed using one of the custommized algorithms as Ring, Butterfly, Nonblocking, Noempty
///
/// The communication algorithm is specified when calling @link MatInit MatInit @endlink or @link MatComShape MatComShape @endlink.
/// An integer encodes all the communication algorithms (None=0, Ring=1, Butterfly=2, Nonblocking=3 Noempty=4).
///
///
///
/// @n
/// \page example Application example 
/// Here is an application example of a least square problem resolution which is implemented in test_pcg_mapmat.c .
/// Considering a problem formulated as \f$ A^t A x = A^t b \f$, Solution x can be compute iteratively using conjutgate gradient.
/// Instead computing and storing the whole \f$A^tA\f$ matrix,
/// we apply succesively pointing, \f$A^t\f$, and unpointing products, \f$A\f$, at each iterate.
/// 
/// Classic gradient conjugate algorithm has been slightly modified.
/// As we explain \f$A^t\f$ and \f$A\f$ are applied succesively. 
/// Furtherwise dotproduct operations in the overlapped domain have been moved in the distributed domain.
/// Espacially  we use relation : \f$< A^t y, x > = < y , Ax > \f$.
///
/// Algorithm needs into memory 6 vectors :
///  - 3 in ovelapped domain(x, gradient, direction),
///  - 3 in distributed domain. 
///  
/// Complexity, counting operations at each iterate, is detailled as follow :
///  - 4 produits scalaires dans le domaine temporelle (communication = single MPI_Allreduce),
///  - 3 axpy dans le domaine de la carte (no communication),
///  - 3 multiplication par \f$A\f$ (no communication),
///  - 1 multiplication par \f$A^t\f$ (communication = MIDAPACK Communication scheme).
///
///\code
/// Mat A;
/// double *x, *g, *d;
/// double *Ax_b, *Ag, *Ad; 
///
/// MatCreate(&A, m, nnz, MPI_COMM_WORLD);	//allocate matrix tabs
/// MatSetIndices(&A, m*nnz, 0, nnz, indices);	//copy indices into matrix structure
/// MatSetValues(&A, m*nnz, 0, nnz, values);    //copy values into matrix structure
///
/// MatLocalShape(&A, SORT_FLAG, OMP_FLAG);	//reindex data structure
///
/// MatComShape(&A, COM_SCHEME_FLAG);		//build communication pattern 
///
/// //conjugate gradient initialization
/// //allocate vectors (overlapped domain) 
/// g  = (double *) malloc(A.lcount*sizeof(double));      //g (gradient)
/// d  = (double *) malloc(A.lcount*sizeof(double));      //d (direction)  
/// //allocate vector (distributed domain) 
/// Ax_b = (double *) malloc(m*sizeof(double));           //Ax_b = Ax-b
/// Ad = (double *) malloc(m*sizeof(double));             //Ad = A d
/// Ag = (double *) malloc(m*sizeof(double));             //Ag = A g
///
/// MatVecProd(&A, x, Ax_b);              //Ax_b = Ax-b
/// for(i=0; i<m; i++)                    // 
///   Ax_b[i] = Ax_b[i]-b[i];             //
///
/// TrMatVecProd(&A, Ax_b, d);            //Ad = A d =  A A^t(Ax-b)
/// MatVecProd(&A, d, Ad);                //
///
/// resnew=0.0;                           //initial residu, resnew = ||A^t(Ax-b)|| = <Ax_b, Ad>
/// localreduce=0.0;                      //
/// for(i=0; i<m; i++)                    //         
///   localreduce+=Ax_b[i]*Ad[i];         //
/// MPI_Allreduce(&localreduce, &resnew, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
///
/// //conjugate gradient iterate
/// for(k=0; k<KMAX ; k++){               //begin loop
///
///   alpha=0.0;                          //alpha = <Ad, Ax_b>
///   localreduce=0.0;                    //
///   for(i=0; i<m; i++)                  //         
///     localreduce+=Ad[i]*Ax_b[i];       //
///   MPI_Allreduce(&localreduce, &alpha, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
///
///   gamma=0.0;                          //gamma = <Ad, Ad>
///   localreduce=0.0;                    //
///   for(i=0; i<m; i++)                  //         
///     localreduce+=Ad[i]*Ad[i];         //
///   MPI_Allreduce(&localreduce, &gamma, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
///
///   for(j=0; j<A.lcount; j++)           // x = x + (alpha/gamma) d 
///     x[j] = x[j] - (alpha/gamma)* d[j];//
///
///   MatVecProd(&A, x, Ax_b);            //Ax_b = Ax-b
///   for(i=0; i<m; i++)                  // 
///     Ax_b[i] = Ax_b[i]-b[i];           //
///
///   TrMatVecProd(&A, Ax_b, g);          //g  = A^t(Ax-b)
///   MatVecProd(&A, g, Ag);              //Ag = AA^t(Ax-b)
///
///   resold=resnew;                      //residu = ||g|| = <Ax-b, Ag>
///   resnew=0.0;                         //
///   localreduce=0.0;                    //
///   for(i=0; i<m; i++)                  //         
///     localreduce+=Ax_b[i]*Ag[i];       //
///   MPI_Allreduce(&localreduce, &resnew, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
///
///   beta=0.0;                           //beta = <Ag, Ad>
///   localreduce=0.0;                    //
///   for(i=0; i<m; i++)                  //         
///     localreduce+=Ag[i]*Ad[i];         //
///   MPI_Allreduce(&localreduce, &beta, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
///
///   if(resnew<tol)			   //convergence test
///      break;
///    
///   for(j=0; j<A.lcount; j++)           //d = -g + (beta/gamma) d
///     d[j]= -g[j] + (beta/gamma)*d[j];  // 
///
///   MatVecProd(&A, d, Ad);              //Ad = A d
/// }
/// \endcode
///
/// More information about pointing operator are detailled, in the @link mapmat.c pointing function synposis @endlink
///
/// @n
